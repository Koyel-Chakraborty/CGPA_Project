\documentclass[12pt,a4paper]{report}
\usepackage{ragged2e}
\usepackage{amsmath,amssymb}
\usepackage{sectsty}
\usepackage{array}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{tocloft, titlesec}




\sectionfont{\LARGE}
\subsectionfont{\Large}
\hypersetup{colorlinks = true, urlcolor = blue, linkcolor = black}
\renewcommand\cftsecfont{\LARGE \bfseries}
\renewcommand\cftsecpagefont{\LARGE}
\renewcommand\cftsecafterpnum{\par\addvspace{20pt}}
\renewcommand\cftsecaftersnumb{\hspace{0.01em}}
\renewcommand\cftsubsecfont{\LARGE}
\renewcommand\cftsubsecpagefont{\LARGE}
\renewcommand\cftsubsecafterpnum{\par\addvspace{20pt}}
\renewcommand\cftsubsecaftersnumb{\hspace{0.01em}}
\renewcommand{\thesection}{\arabic{section}}

\begin{document}
\title{\vspace{-105pt}\hspace{-140pt}\textit{\small{M.Sc. in Applied Statistics and Analytics}}\\\vspace{80pt}\textit{Project Report\\on}\\\textbf{\huge{Students' Performance in Online Examination: The Gender Perspective}}\vspace{-15pt}}
\date{}
\author{\textit{\Large{Submitted by,}}\\[4pt]
\begin{tabular}{cc}
    \hline
    \hline
    \\[-3pt]
    \multicolumn{2}{c}{\textbf{Group: 6}}\\
    \multicolumn{2}{c}{\textit{Semester: 1}}\\[8pt]
    \quad \Large{Abhik De} & \Large{Anubhab Maity}\\
    \quad \Large{Koyel Chakraborty} & \Large{Kushaal Agarwal}\\
    \quad \Large{Soumitro Mukherjee} & \Large{Susnato Chakraborty}\\[12 pt]
    \hline
    \hline
\end{tabular}
\\[70 pt]
\textit{\Large{Supervised by,}}\\[10 pt]
\Large{Mr. Taranga Mukherjee,}\\
\large{\textit{Assistant Professor,}}\\
\large{\textit{Department of Applied Science}}\\
\vspace{66pt}\\
\large{Maulana Abul Kalam Azad University of Technology}}
\maketitle
\tableofcontents
\pagebreak
\section{Introduction}
\justifying
\Large{When the recent pandemic was drastically transforming our socio-economic acumen, the academia faced a prominent paradigm shift. The initiation of virtual classrooms, whether a boon or a bane, is still a matter of debate for many. Moreover, online evaluation systems have received massive appreciation and criticism both at the same time. In this study, we intend to investigate how flexible the new framework is in granting decent grades. Moreover, our motivation is to examine the notions of women emancipation in terms of students' adaptability at the face of technological infusion. If such gender prerogatives are actually valid or not, can be deduced from the performance of students during the online examination.}
\section{Database and Methodologies}
\subsection{Source of Data}
\justifying
\Large{Primary data have been collected from the first semester students of M.Sc. in Applied Statistics and Analytics course conducted by the Maulana Abul Kalam Azad University of Technology, West Bengal. Our dataset consists of undergraduate scores (CGPAs) of 30 respondents from our class. The aforementioned undergraduate course had the duration of six consecutive semesters (2018-2021). During the course, all semester examinations except the first and second one were conducted online. In this study, aggregate score (CGPA) of all the semesters taken together will be used. Based on this sample, we are interested to infer if the scores are significantly varied across gender.}
\subsection{Research Questions}
\justifying
\begin{enumerate}
   \item[\Large{\textbf{\textit{1.}}}]
   \Large{Is the overall grade more than 7.5 pointer?}
   \item[\Large{\textbf{\textit{2.}}}]
   \Large{Did the female students perform better than their male peers in terms of scoring \large{CGPA}?}
   \item[\Large{\textbf{\textit{3.}}}]
   \Large{Are the female examinees more consistent than the male aspirants appearing in the examination?}
\end{enumerate}
\subsection{Methodologies}
\justifying
\subsubsection{\large{Exploratory Data Analysis}}
\Large{To check if the normality assumption is valid for the population under study, we have employed exploratory data analysis techniques on our random sample. These exploratory data analysis techniques include Histogram, Kernel Density Plot, Boxplot and Quantile-Quantile Plot.}
\vspace{-30pt}
\subsubsection{\large{Hypothesis Tests}}
\Large{Parametric hypothesis tests (One Sample t-Test, Fisher's t-Test, F-test) as well as their non-parametric counterparts (Sign Test, Wilcoxon Signed-Rank Test, Mann-Whitney U Test, Ansari-Bradley Test) have been used in the study. To draw a final conclusion about the normality behaviour of the data, Kolmogorov-Smirnov Test has been applied prior to these tests. }\\[20pt]
\Large{{\textbf{\textit{1. Kolmogorov-Smirnov Test: }}} }\\[10pt]
\Large{This test is used to check if a sample comes from a particular distribution. Under the null hypothesis, it tests the equality of the empirical distribution with that of the theoretical one. According to the assumption, the theoretical population must have an absolutely continuous distribution function. If $F_n(x)$ and $F(x)$ be the empirical and theoretical  CDFs respectively, we define the test statistic as,}\\
\begin{equation}
W = \underset{x\in \mathbb{R}}{Sup} |F_n(x) - F_(x)|
\end{equation}\\
\large{\textit{{\textbf{Note.}} Here, $\mathbb{R}$ denotes the set of all real numbers. $Sup$ is an abbreviation for Supremum.}}\\[10pt]
\Large{When the null hypothesis is true, i.e., if the empirical distribution approaches the theoretical distribution, the test statistic approaches to zero.}\\[20 pt]
\Large{{\textbf{\textit{2. Student's t-Test: }}} }\\
\Large{Originally devised by William Sealy Gosset in 1908, t-Test is used as an one-sample location test. One principle assumption is that, the sampling distribution is normal. It is a mean-based test, i.e, it compares the sample mean with a hypothesized population mean value. The test statistic,}\\
\begin{equation}
T = \frac{\bar{x} - \mu}{\frac{S}{\sqrt{n}}}\\
\end{equation}\\[8pt]
\Large{follows a \textit{t}-distribution with ($n-1$) degrees of freedom.}\\[6pt]
\large{\textit{{\textbf{Note.}} Here, $\bar{x}$ represents sample mean, $\mu$ denotes the population mean, S is the sample standard deviation and n denotes the sample size.}}\\[20pt]
\Large{{\textbf{\textit{3. Fisher's t-Test: }}} }\\
\Large{This is a special version of the t-Test to compare the location parameters of two sample groups. One additional assumption is that, the observations are independent within and across groups. Besides, the two groups are required to be homoscedastic. It is a mean-based test, i.e, it compares the sample means of the two groups. The test statistic,}\\
\begin{equation}
T = \frac{(\bar{x_1}-\bar{x_2}) - (\mu_1 -\mu_2)}{S_p(\sqrt{\frac{1}{n_1} + \frac{1}{n_2}})}\\
\end{equation}\\[10pt]
\Large{follows a \textit{t}-distribution with ($n_1 + n_2 - 2$) degrees of freedom.}\\
\begin{center}
where, ${S_p}^2 = \frac{(n_1 - 1){S_1}^2 + (n_2 - 1){S_2}^2}{n_1 + n_2 - 2}$\\[25pt]
\end{center}
\large{\textit{{\textbf{Note.}} Here, $\bar{x_1},\bar{x_2}$ represents sample means of two groups under study, $\mu_1,\mu_2$ denotes the population means of the respective groups, $S_p$ is the pooled sample standard deviation and $n_1,n_2$ denotes the sample sizes.}}\\[6pt]
\Large{In case, the two populations under study are not homoscedastic, the test can be performed applying Satterthwaite approximation (Satterthwaite, 1946). This test is also termed as \textbf{\large{Welch Test.}} The test statistic $T'$ is given by,}\\[6pt]
\begin{equation}
T' = \frac{(\bar{x_1}-\bar{x_2}) - (\mu_1 -\mu_2)}{\sqrt{\frac{{S_1}^2}{n_1} + \frac{{S_2}^2}{n_2}}}\\
\end{equation}\\[10pt]
\Large{which follows a \textit{t}-distribution with v degrees of freedom.}\\[0.5pt]
\begin{center}
where, $v = \frac{(\frac{{S_1}^2}{n_1}+\frac{{S_2}^2}{n_2})^2}{{(n_1 - 1){(\frac{{S_1}^2}{n_1}})^2} + {(n_2 - 1){(\frac{{S_2}^2}{n_2}})^2}}$\\[20pt]
\end{center}
\large{\textit{{\textbf{Note.}} Here, $\bar{x_1},\bar{x_2}$ represents sample means of two groups under study, $\mu_1,\mu_2$ denotes the population means of the respective groups, $S_1,S_2$ is the sample standard deviations of the two groups and $n_1,n_2$ denotes the sample sizes.}}\\[30pt]
\Large{{\textbf{\textit{4. F-Test: }}} }\\[10 pt]
\Large{Initially developed by R. A. Fisher in the 1920's, the nomenclature was given by George W. Snedecor. It is a variance-based test, i.e, it compares the sample variances of two normal populations. The test statistic,}\\[0.5pt]
\begin{equation}
F = \frac{{\sigma_2}^2{S_1}^2}{{\sigma_1}^2{S_2}^2}\\
\end{equation}\\[0.5pt]
\Large{If the null hypothesis is $H_0:\sigma_1 = \sigma_2$, then the above expression reduces to,}\\[0.1pt]
\begin{equation}
F = \frac{{S_1}^2}{{S_2}^2}\\
\end{equation}\\[0.1pt]
\Large{The test statistic follows a \textit{F}-distribution with $v_1=n_1-1$ and $v_2=n_2-1$ degrees of freedom.}\\[10pt]
\large{\textit{{\textbf{Note.}} Here, $S_1,S_2$ represents sample standard deviations of two groups under study and $\sigma_1,\sigma_2$ denotes the population standard deviations of the respective groups. $n_1,n_2$ denotes the sample sizes.}}\\[20pt]
\Large{{\textbf{\textit{5. Sign Test: }}} }\\[10pt]
\Large{Initially developed by John Arbuthnot in 1710, the test is a non-parametric version of one sample t-Test. It is a median-based test, i.e, it compares the sample median with the hypothesized population median value, $\tilde{\mu}$. According to the assumption, the population must have an absolutely continuous distribution function. For this, all sample observations exactly equal to the hypothesized median value needs to be excluded from the analysis. The test procedure comprises of calculating the differences, $d_i = x_i-\tilde{\mu}$ for each sample value $x_i$. Then, the indicator variable $Z_i$ is defined as,}\\[10pt]
\begin{equation*}
Z_i = 
 \begin{cases}
 1, & \text{if $d_i>0$}\\
 0, & otherwise
 \end{cases}
\end{equation*}\\[10pt]
\Large{The test statistic is given by,}
\begin{equation}
  S_+ = \sum_{i=1}^{n} {Z_i}  
\end{equation}
\Large{Then probability of getting less than (or more than, depending on the alternative hypothesis) $S_+$ successes is calculated from a binomial distribution with parameters $n (\textit{number of observations}\neq\tilde{\mu})$ and $p=0.5$. This probability is used as the $p-Value$ of the test.}\\[20pt]
\Large{{\textbf{\textit{6. Wilcoxon Signed-Rank Test: }}} }\\[10pt]
\Large{This test was devised by Frank Wilcoxon in 1945. The test is a non-parametric version of one sample t-Test and a modified version of the sign test. It is a median-based test, i.e, it compares the sample median with the hypothesized population median value. According to the assumption, the population must have an absolutely continuous distribution function. The test procedure comprises of calculating $d_i = (x_i-\tilde{\mu})$ for each sample value $x_i$ and subsequently ranking $|d_i|$'s. The test statistic,}\\[20pt]
\Large{\textit{\textbf{\underline{for left-tailed test:}}}}\\[0.5pt]
\begin{equation}
W_+ = \sum_{i} R_i \quad \forall \ i \ \ni \ d_i>0
\end{equation}\\
\Large{\textit{\textbf{\underline{for right-tailed test:}}}}\\[0.5pt]
\begin{equation}
W_- = \sum_{i} R_i \quad \forall \ i \ \ni \ d_i<0 
\end{equation}\\
\Large{\textit{\textbf{\underline{for both-tailed test:}}}}\\[0.5pt]
\begin{equation}
W = min\{W_+,W_-\}
\end{equation}\\
\large{\textit{{\textbf{Note.}} Here, $R_i$ represents the i'th rank. Prior to the test, all observations resulting in tie ranks and zero values of $|d_i|$ must be excluded from the analysis.}}\\[10pt]
\Large{If the computed test statistic is less than the tabulated value of $W_+$ (for left-tailed test), $W_-$ (for right-tailed test) or $W$ (for both-tailed test) at the corresponding sample size n and level of significance $\alpha$, the null hypothesis is rejected.}\\[20pt]
\Large{{\textbf{\textit{7. Mann-Whitney U Test: }}} }\\[10pt]
\Large{This test was proposed by Frank Wilcoxon in 1945. Later, Henry Mann and Donald Ransom Whitney (1947) performed a detailed analysis of the test. For this reason, the test is also called Mann-Whitney-Wilcoxon Test. It is a non-parametric version of the independent samples t-Test or Fisher's t-Test. Inferences are drawn about the location parameters of the two populations under study. According to the assumption, the populations must have an absolutely continuous distribution function. To perform the test, we define an indicator variable,}\\
\begin{center}
\begin{math}
\Phi(X_i,Y_j) = \left\{
\begin{array}{lll}
1, & \mbox{if $X_i>Y_j$} & \mbox{\quad $i = 1(1)n_1$}\\
0, & \mbox{otherwise} & \mbox{\quad $j = 1(1)n_2$}
\end{array}\right.
\end{math}\\[20pt]
\end{center}
\Large{The test statistic is given by,}\\[0.5pt]
\begin{equation}
U = \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} \Phi(X_i,Y_j)
\end{equation}\\
\large{\textit{{\textbf{Note.}} Here, X, Y denotes the two populations under study. $X_i$ denotes the $i$'th observation in the random sample taken from population X. $Y_j$ denotes the $j$'th observation in the random sample taken from population Y.}}\\[10pt]
\Large{When C is the tabulated critical value of U at the specified level of significance $\alpha$ and combined sample size ($n_1 + n_2$), we reject null hypothesis if $U > C$ for right tailed test. For left tailed test, we reject null hypothesis if $U < C$. For both tailed test, any of the two conditions needs to be satisfied at $\alpha/2$ level for acceptance of the alternative hypothesis. }\\[20pt]
\Large{{\textbf{\textit{8. Ansari-Bradley Test: }}} }\\[10pt]
\Large{It is a non-parametric version of the Fisher's F Test. Inferences are drawn about the scale parameters (variance) of the two populations under study. Under the null hypothesis, it tests the equality of the scale parameters. According to the assumption, the populations must have an absolutely continuous distribution function. If X and Y be two populations under study, we define an indicator variable,}\\
\begin{center}
\begin{math}
Z_i = \left\{
\begin{array}{ll}
1, & \mbox{if i'th combined sample }\\
   & \mbox{is from population $X$}\\
0, & \mbox{Otherwise}
\end{array}\right.
\end{math}\\[20pt]
\end{center}
\Large{The linear rank statistic is given by,}\\[0.5pt]
\begin{equation}
A_N = \sum_{i=1}^{n_1} |i - \frac{N+1}{2}|.Z_i
\end{equation}\\
\large{\textit{{\textbf{Note.}} Here, N denotes the combined sample size ($n_1$ + $n_2$)}}\\[10pt]
\Large{When C is the tabulated critical value of $A_N$ at the specified level of significance $\alpha$ and combined sample size ($n_1 + n_2$), we reject null hypothesis if $A_N > C$ for right tailed test. For left tailed test, we reject null hypothesis if $A_N < C$. For both tailed test, any of the two conditions needs to be satisfied at $\alpha/2$ level for acceptance of the alternative hypothesis. }\\[20pt]
\Large{\textit{Table 1} outlines the application of parametric and non-parametric hypothesis tests in context of the present study.}\\[10pt]
\begin{center}
\Large{\textit{Table 1: Application of Hypothesis Tests w.r.t. Research Questions}}
\begin{table}[hbt!]
\Centering
\Large
\begin{tabular}{c|c|c|c}
\hline
Research & Test & Parametric & Non-Parametric\\
Questions & Type & Tests & Tests\\
\hline
 &&&\\
  & One & & Sign\\
1  & Sample & Student's t &  \\
  & Location &   & Wilcoxon Signed Rank\\
  &&&\\
\hline
  & Two & & \\
2  & Sample & Fisher's t &  Mann-Whitney U\\
  & Location &   & \\
\hline
  & Two & & \\
3  & Sample & Fisher's F & Ansari-Bradley \\
  & Variance &   & \\
\hline
\end{tabular}
\end{table}
\end{center}

\pagebreak
\section{Results and Discussion}
\justifying
\Large{Prior to the analysis, it is important to study the descriptive statistics. For this purpose, two summary tables of CGPA scores have been presented below.}
\begin{center}
\Large{\textit{Table 2: Basic Features of the Data}}
\begin{table}[hbt!]
\Large
\begin{tabular}{c|c|c|c}
\hline
Group & Sample Size & Mean & Standard Deviation\\
\hline
Male & 17 & 7.242 & 0.688\\
Female  & 13 & 7.589 & 0.367\\
Combined & 30 & 7.393 & 0.589\\
\hline
\end{tabular}
\end{table}
\end{center}
\begin{center}
\Large{\textit{Table 3: Five Number Summary}}
\begin{table}[hbt!]
\Large
\begin{tabular}{c|c|c|c|c|c}
\hline
Group & Minimum & $Q_1$ & $Q_2$ & $Q_3$ & Maximum\\
\hline
Male & 5.300 & 7.000 & 7.400 & 7.520 & 8.500\\
Female & 7.001 & 7.300 & 7.500 & 7.900 & 8.120\\
Combined & 5.300 & 7.100 & 7.442 & 7.685 & 8.500\\
\hline
\end{tabular}
\end{table}
\end{center}
\large{\textit{{\textbf{Note.}} Here, $Q_1, Q_2, Q_3$ denotes the first, second and third quartiles respectively.}}\\[10pt]
\Large{Clearly the sample indicates that, on average, female students have performed better than their male peers. Additionally, the performance of the female students are comparatively less varied. But, to examine the significance of such claim at population level, we need to perform hypothesis tests. For this purpose, we need to know if normality assumption is valid for the population under study.}\\
\Large{From \textit{Figure 1, Figure 2} and \textit{Figure 3}, following interpretations can be deduced,}
\begin{enumerate}
   \item[\Large{\textbf{\textit{1.}}}]
   \Large{The male students' sample as well as the combined sample contain outliers.}
   \item[\Large{\textbf{\textit{2.}}}]
   \Large{Symmetricity is not largely compromised in any of the Histograms and KDE (Kernel Density Estimation) Plots. But, the Box Plot of male sample detects considerable difference between mean and median.}
   \item[\Large{\textbf{\textit{3.}}}]
   \Large{In Quantile-Quantile Plots, no foolproof evidence of normailty has been found.}
\end{enumerate}
\Large{For a more formal conclusion about normality behaviour, \textit{Kolmogorov-Smirnov Test} has been applied with respect to the following hypotheses,}\\[5pt]
\Large{\textit{\textbf{Null hypothesis:}}}
\begin{equation*}
 H_0: P = P_N   
\end{equation*}
\Large{\textit{\textbf{Alternative hypothesis:}}}
\begin{equation*}
 H_1: P \neq P_N   
\end{equation*}
\large{\textit{{\textbf{Note. }}Here, P denotes the empirical distribution under study and $P_0$ denotes normal distribution. The findings of the test have been summarized in Table 4.}}
\pagebreak
\begin{center}
\begin{figure}[hbt!]
    \centering
    \vspace{-30pt}
    \Large{\textit{\textbf{\hspace{12pt} CGPA Scores}}}\\[30pt]
    \includegraphics[width = 15 cm]{main2/Combined2.png}
    \caption{Exploratory Analysis of CGPA Scores}
    \label{fig: cgpa}
\end{figure}
\end{center}
\pagebreak
\begin{center}
\begin{figure}[hbt!]
    \centering
    \vspace{-30pt}
    \Large{\textit{\textbf{\hspace{25pt}CGPA Scores (Genderwise)}}}\\[30pt]
    \includegraphics[width = 15 cm]{main2/gender_eda_1_cropped.png}
    \caption{Histograms and KDE Plots of Male and Female Group}
    \label{fig: cgpa}
\end{figure}
\end{center}
\pagebreak
\begin{center}
\begin{figure}[hbt!]
    \centering
    \vspace{-30pt}
    \Large{\textit{\textbf{\hspace{25pt}CGPA Scores (Genderwise)}}}\\[30pt]
    \includegraphics[width = 15 cm]{main2/gender_eda_2_cropped.png}
    \caption{Box Plots and Q-Q Plots of Male and Female Group}
    \label{fig: cgpa}
\end{figure}
\end{center}
\pagebreak
\begin{center}
\Large{\textit{Table 4: Results of Kolmogorov-Smirnov Test}}\\
\begin{table}[hbt!]
\Centering
\Large
\begin{tabular}{c|c|c}
\hline
\quad Sample \quad & \quad Test Statistic \quad & \quad p-Value \quad\\
\hline
\quad\quad Male \quad\quad & \quad 0.15115 \quad & \quad\quad 0.8341 \quad\quad\\
\quad\quad Female \quad\quad & \quad 0.12001 \quad & \quad\quad 0.9868 \quad\quad\\
\quad\quad Combined \quad\quad & \quad 0.11137 \quad & \quad\quad 0.8548 \quad\quad\\
\hline
\end{tabular}
\end{table}
\end{center}
\Large{\textit{Table 4} clearly depicts that, null hypotheses can not be rejected for any of the populations under study. Thus, no significant evidence of non-normality has been found. For this reason, we can safely incorporate parametric tests in our study. But, as a buffer against probable \textit{Type II error} of Kolmogorov-Smirnov Test, non-parametric tests have also been conducted. \textit{Table 5} contains the set of hypotheses used in the study.}
\begin{center}
\Large{\textit{Table 5: Declaration of Hypotheses w.r.t Reasearch Questions}}\\[10 pt]
\begin{tabular}{c|c|c}
\hline
\quad Research \quad & \quad Null \quad & \quad Alternative \quad\\
\quad Questions \quad & \quad Hypothesis \quad & \quad Hypothesis \quad\\
\hline
\hline
\quad 1 \quad & \quad $H_0:\mu_0 = 7.5$ \quad & \quad $H_1:\mu_0 > 7.5$ \quad\\
\quad 2 \quad & \quad $H_0:\mu_F = \mu_M$ \quad & \quad $H_1:\mu_F > \mu_M$ \quad\\
\quad 3 \quad & \quad $H_0:\sigma_F = \sigma_M$ \quad & \quad $H_1:\sigma_F < \sigma_M$ \quad\\
\hline
\end{tabular}\\[20pt]
\end{center}
\large{\textit{{\textbf{Note.}} Here, $\mu_0$ represents the center of the population (CGPAs scored by the students), $\mu_M,\mu_F$ denote centers of the male and female populations under study. $\sigma_M,\sigma_F$ are the respective standard deviations.}}\\
\Large{Based on our methodological framework, hypothesis tests have been performed. The results regarding the same have been summarized in \textit{Table 6}.}\\
\begin{center}
\Large{\textit{Table 6: Results of Parametric and Non-Parametric Tests}}\\
\begin{table}[hbt!]
\Centering
\Large
\begin{tabular}{c|c|c}
\hline
Test & $H_1$ & $p-Value$\\
\hline
\hline
Student's t & $H_1:\mu_0 > 7.5$ & $0.8369$\\
\hline
Sign Test & $H_1:\mu_0 > 7.5$ & $0.9075$\\
\hline
Wilcoxon Signed Rank & $H_1:\mu_0 > 7.5$ & $0.6925$\\
\hline
Fisher's t \small{(Equal Variance)} & $H_1:\mu_F > \mu_M$ & $0.0557$\\
\hline
Fisher's t \small{(Unequal Variance)} & $H_1:\mu_F > \mu_M$ & $0.0439$\\
\hline
Mann-Whitney U & $H_1:\mu_F > \mu_M$ & $0.0434$\\
\hline
Fisher's F & $H_1:\sigma_F < \sigma_M$ & $0.0163$\\
\hline
Ansari-Bradley & $H_1:\sigma_F < \sigma_M$ & $0.2699$\\
\hline
\end{tabular}
\end{table}
\end{center}
\large{\textit{{\textbf{Note.}} Fisher's t Test has been done twice, one with equal variance and another with unequal variance assumption. $H_1$ denotes the alternative hypotheses respective to the tests. }}\\[10pt]
\Large{In this study, level of significance ($\alpha$) has been fixed at 0.05. From \textit{Table 6}, it is clear that,}
\begin{enumerate}
    \item [\textit{\textbf{1.}}]
    \Large{Alternative hypothesis $H_1:\mu_0 > 7.5$ can not be accepted for any of the tests under study. }
     \item [\textit{\textbf{2.}}]
    \Large{Alternative hypothesis $H_1:\mu_F > \mu_M$ can be accepted for Fisher's t Test with unequal variance assumption and also for Mann-Whitney U Test. But, Fisher's t Test with equal variance assumption fails to reject $H_0$.}
    \item [\textit{\textbf{3.}}]
    \Large{Alternative hypothesis $H_1:\sigma_F < \sigma_M$ can be accepted for Fisher's F Test. But, it's non-parametric alternative, i.e., Ansari-Bradley Test could not reject $H_0$. Since, the parametric version is more powerful and as our data have not shown any significant non-normal behaviour, we accept the verdict of F test in this case.}
\end{enumerate}
\section{Concluding Remarks}
\Large{Thus, we reach at the following final remarks,}
\begin{enumerate}
    \item [\textit{\textbf{1.}}]
    \Large{In this study, we could not find sufficient evidence of average CGPA being more than 7.5 pointer.}
     \item [\textit{\textbf{2.}}]
    \Large{On average, female students performed even better than their male peers.}
    \item [\textit{\textbf{3.}}]
    \Large{Female examinees are found to be even more consistent in their performance than male candidates of the examination.}
\end{enumerate}
\pagebreak
\section{Conclusion}
\Large{Thus, the findings of the study suggest that, emancipation of women has erased all concepts of feminine inferiority in terms of technological advancements. Rather, women have effortlessly habituated themselves with the new evaluation regime, so much so that they could supersede men. The reason can be attributed to their inherent capability of adaptation and naturalization. The study culminates on a positive note that, technology could not become a barrier against this exquisite quality of women.}
\section{References}
\textbf{\textit{\Large{Books}}}\\
\large{\textit{\textbf{1.} Casella, G., \& Berger, R. L. (2021). Statistical inference. Cengage Learning.}}\\[5pt]
\large{\textit{\textbf{2.} Hogg, R. V., McKean, J., \& Craig, A. T. (2005). Introduction to mathematical statistics. Pearson Education.}}\\[5pt]
\large{\textit{\textbf{3.} Rohatgi, V. K., \& Saleh, A. M. E. (2015). An introduction to probability and statistics. John Wiley \& Sons.}}\\[5pt]
\large{\textit{\textbf{4.} Walpole, R. E., Myers, R. H., Myers, S. L., \& Ye, K. (1993). Probability and statistics for engineers and scientists (Vol. 5). New York: Macmillan.}}\\[10pt]
\textbf{\textit{\Large{Web Links}}}\\
\large{\textit{\textbf{1.} \url{https://www.rdocumentation.org/}}}\\[5pt]
\large{\textit{\textbf{2.} \url{https://www.statsmodels.org/}}}\\[5pt]
\large{\textit{\textbf{3.} \url{https://docs.scipy.org/doc/scipy/}}}
\pagebreak
\begin{center}
\textit{\vspace{7cm}\\\Large{\underline{\textbf{Submitted on:} January 13, 2022}}}
\end{center}
\pagebreak
\end{document}
